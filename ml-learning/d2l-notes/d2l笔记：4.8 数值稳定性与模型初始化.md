# d2l笔记：4.8 数值稳定性与模型初始化

## 模型中的一些值得注意的问题

选择激活函数与初始化参数是很重要的，对数值的稳定性起着举足轻重的作用。不当的选择可能会导致梯度消失与梯度爆炸，为调参制造极大的麻烦甚至影响最后的训练是否成功。

### 梯度消失与梯度爆炸

在层数较多时，各种趋势都会聚沙成塔，最终造成极大的效果。

例如，当sigmoid 函数 $\frac{1}{1+\exp(-x)}$ 的输入很大或很小时，其梯度都会消失。在网络有很多层的情况下，要保证不发生这种情况是很难的，所以我们更倾向于选择更加稳定的ReLU函数。

再如，在计算梯度时，许多矩阵相乘的情况是极其常见的，但这时，一个不大的方差就会导致最后的结果产生梯度爆炸。

### 对称性

当初始化参数相同时，神经网络的几个单元有可能在训练过程中同步变化，由于接收到的信息与做出的反馈是一模一样的，所以会一直同步下去，导致网络的表达能力永远也不会被充分发挥出来。隐藏层的行为好像只有一个单元。

暂退法和随机初始化可以打破这种对称性。

## 让训练更加稳定

目标：让梯度值在合理的范围内

途径：

将乘法变加法 (如 ResNet, LSTM)

归一化（梯度归一化，梯度裁剪）

合理的初始权重和激活函数

## 参数初始化

通过参数初始化和适当的正则化，可以针对上述问题做出改善。

### 默认初始化

对于中等难度的问题，使用框架默认的随机初始化方法通常很有效。

### Xavier初始化

如果每层神经网络的权重参数是随机正态分布，随着层数的增加，输出的方差会急剧变小（这也和梯度爆炸、梯度消失有关）。但是 Xavier 初始化可以很好地解决这个问题。

Xavier初始化通过一些技巧，使得每一层输出的方差与输入的方差较为接近。

在应用中，我们可以直接调用 Pytorch 中提供的方法 `torch.nn.init.xavier_uniform_` 或 `torch.nn.init.xavier_normal_` 来使用 Xavier 初始化。

# d2l笔记：4.9 环境和分布偏移

## 分布偏移的类型

### 协变量偏移

### 标签偏移

### 概念偏移

## 分布偏移的纠正

### 协变量偏移纠正

### 标签偏移纠正

### 概念偏移纠正

## 学习问题的分类法

### 批量学习

batch learning 系统不会再进行更新

### 在线学习

通过新的观测结果不断改进模型。

### 老虎机

只有有限数量的手臂可以拉动。

### 控制

PID 控制器算法

### 强化学习

### 考虑到环境

