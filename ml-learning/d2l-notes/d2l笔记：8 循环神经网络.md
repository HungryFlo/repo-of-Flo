# d2l笔记：8 循环神经网络

## 序列模型

序列的数据有一些自己的特点，我们需要不断通过过去的数据来对新数据进行预测。

**自回归模型**：取一定长的观测序列训练一个网络。通俗来讲就是说预测明天的数据，就只考虑前天、昨天、今天的数据，舍弃前天之前的数据。

**隐变量自回归模型**：每次都生成一个对过去观测的总结，基于总结来进行预测。也就是说我每天都回顾总结一下前面所有时间的经验，这样就可以考虑到所有之前的数据。

如果一个序列可以通过自回归模型（上面所说的第一种）计算出精确的预测，那么这个序列就满足**马尔可夫条件**，也就是说，下一项的值可以只通过前面有限的 n 项进行计算。 阶数越高，对应的依赖关系就越长。 

特别的，如果 n=1 ，那么就可以得到一个**一阶马尔可夫模型**，也即该序列只要知道上一项就可以算出下一项。

## 文本预处理

解析文本的步骤：

1. 将文本作为字符串加载到内存中。
2. 将字符串拆分为词元（如单词和字符）。
3. 建立一个词表，将拆分的词元映射到数字索引。
4. 将文本转换为数字索引序列，方便模型操作。

### 读取数据集

可以使用正则表达式等工具对加载的文本进行控制和整理。

### 词元化

词元（token）是文本的基本单位，每个词元都是一个字符串。

### 词表

*词表*（vocabulary）用来将字符串类型的词元映射到从0开始的数字索引中。

将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为*语料*（corpus）。 

然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。

另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 

我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。

## 语言模型和数据集

### 马尔可夫模型与n元语法

语言是一种序列，上面的马尔可夫模型可以应用于语言建模问题。

通常，涉及一个、两个和三个变量的概率公式分别被称为 *一元语法*（unigram）、*二元语法*（bigram）和*三元语法*（trigram）模型。 

### 自然语言统计

通过词频图，我们可以分析出一下的现象：

词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足*齐普夫定律*（Zipf’s law）， 即第$i$ 个最常用单词的频率 $n_i$ 为：
$$
n_i \propto \frac{1}{i^{\alpha}}
$$
两边取对数
$$
\log n_{i} = - \alpha \log i + c
$$
统计一些词元的频率，结果如下：

<img src="F:\repo-of-Flo\ml-learning\d2l-notes\d2l笔记：8 循环神经网络.assets\image-20240318173721296.png" alt="image-20240318173721296" style="zoom:50%;" />

1. 除了一元语法词，单词序列似乎也遵循齐普夫定律， 指数的大小受序列长度的影响；
2. 词表中 $n$ 元组的数量并没有那么大，这说明语言中存在相当多的结构， 这些结构给了我们应用模型的希望；
3. 很多 $n$ 元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。 作为代替，我们将使用基于深度学习的模型。

### 读取长序列数据

我们可以从随机偏移量开始划分序列， 以同时获得*覆盖性*（coverage）和*随机性*（randomness）。 下面，我们将描述如何实现*随机采样*（random sampling）和 *顺序分区*（sequential partitioning）策略。

#### 随机采样

每个样本都是在原始的长序列上任意捕获的子序列。

在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。

## 循环神经网络理论基础

### 具有隐状态的循环神经网络的循环层与输出层

一种含有隐状态的循环神经网络的循环层定义如下：

（$\phi$ 为激活函数）
$$
H_t = \phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)
$$
也即 **当前时间步的隐藏变量** 是 **当前时间步的输入** 和 **前一个时间步的隐藏变量** 的函数。

输出层：
$$
O_t = H_t W_{hq} + b_q
$$
上面的矩阵的形状如下：

$n$ 为批量大小

$d$ 为输入维度

$h$ 为隐藏单元的数目
$$
X_t \in \R^{n \times d} \\
W_{xh} \in \R^{d \times h} \\
H_t \in \R^{n \times h} \\
W_{hh} \in \R^{h \times h} \\
b_h \in \R^{1 \times h} \\
b_q \in \R^{1 \times h}
$$
不同的时间步使用的模型参数（权重和偏置的值）是相同的，所以循环神经网络的参数开销不会随时间步的增加而增加。

<img src="F:\repo-of-Flo\ml-learning\d2l-notes\d2l笔记：8 循环神经网络.assets\image-20240319223201626.png" alt="image-20240319223201626" style="zoom:50%;" />

在隐藏层中的运算还可以进行化简，$X_tW_{xh}+H_{t-1}W_{hh}$ 的运算相当于 $X_t$ 和 $H_{t-1}$ 的拼接与$W_{xh}$ 与 $W_{hh}$ 拼接的矩阵乘法。（ $\R^{n \times (d+h)} \times \R^{(d+h) \times h} = \R^{n \times h}$ ，表示矩阵乘法的形状变化情况，符号使用确实不是很严谨）

### 基于循环神经网络的字符级语言模型

词元的分割可以是单词，也可以是单个字符。

### 模型质量的度量——困惑度（Perplexity)

一个更好的语言模型应该能让我们更准确地预测下一个词元。 因此，它应该允许我们在压缩序列时花费更少的比特。 所以我们可以通过**一个序列中所有的 $n$ 个词元的交叉熵损失的平均值**来衡量：
$$
\frac{1}{n} \sum \limits_{t=1}^n -\log P(x_t | x_{t-1}, ..., x_1)
$$
由于历史原因，自然语言处理的科学家更喜欢使用一个叫做**困惑度（perplexity）**的量，也就是上面式子的 e 指数。
$$
\exp (-\frac{1}{n} \sum \limits_{t=1}^n \log P(x_t | x_{t-1}, ..., x_1))
$$

- 在最好的情况下，模型总是完美地估计标签词元的概率为1。 在这种情况下，模型的困惑度为1。
- 在最坏的情况下，模型总是预测标签词元的概率为0。 在这种情况下，困惑度是正无穷大。

## 循环神经网络的从零开始实现

### 导入相关的包

```python
import math
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
```

### 加载数据集

```python
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

### 初始化模型参数

normal 的同时把张量送到指定的设备上去（CPU或GPU）。

各个量的形状参考上面的理论分析进行设置。

```python
def get_param(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)

    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)

    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```

### 循环神经网络模型

为了定义循环神经网络模型， 我们首先需要一个`init_rnn_state`函数在初始化时返回隐状态。

```python
def init_rnn_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),)
```

下面的函数用于计算该时间步的隐状态和输出：

```python
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量大小，词表大小)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # X的形状：(批量大小，词表大小)
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```

创建一个类来包装这些函数，并存储模型的参数：

```python
class RNNModelScratch:
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn
        
    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)
    
    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
```

### 预测

下面定义预测函数来生成新字符。

在初始时模型中的隐状态等值并不太适合直接输出结果，需要我们先进行**预热（warm-up）**，预热期结束之后，再开始预测字符并将其输出。

