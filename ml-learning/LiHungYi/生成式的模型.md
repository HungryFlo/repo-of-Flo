# 生成式模型Generative Adversarial Network(GAN)

## 引入

之前的网络都是输入一个 x 输出一个 y（函数），但生成式的模型的输入除了 x 还有一个随机的 z ，这个 z 是从某个简单分布中取样出来的，我们知道 z 的分布是怎样的。 x 和 z 经过网络后会输出一个较复杂的分布。

当我们的任务需要一点“创造力”的时候，也就是对于同样的输入会有不同的输出，就适合用这种输出概率分布的网络。

## GAN 的基本思想

### Unconditional generation

不输入 x ，只输入 z 。

z 是一个较低维的向量，通过Generator之后会生成一个高维向量的分布。

### Discriminator

输入一张图片，输出一串数字。

### 基本思想

类似于捕食者（鸟）和被捕食者（枯叶蝶）的不断进化，我们可以使用 Generator（被捕食者） 和 Discriminator（捕食者）的关系来共同进步。或者比喻成做假钞的人和识别假钞的警察。

### 算法

初始化 generator 和 discriminator

对于每一次训练循环：

1. 固定 generator G，训练 discriminator D，把数据库中的“真图片”和 G 产生的图片做区分。
2. 固定 D，更新 G，使其骗过 D（把G和D直接接起来，把D的输出作为整个模型的输出，只调整G的部分使得最终输出越大越好。

## 理论

### 我们的目标

我们的训练目标：模型生成的分布和真正数据集的分布越相似越好。

Loss Function:
$$
G^* = \arg \min \limits_{G} Div(P_G, P_{data})
$$
$Div$ : Divergence between distributions $P_G$ and $P_{data}$

那么如何计算 Divergence 呢？

GAN 提供了一个思路：不用非得比较整个分布，有采样样本就可以啦！只需要使用 Discriminator。

训练目标：
$$
D^* = \arg \max \limits_D V(D,G)
$$
目标函数：
$$
V(G,D)=E_{y \sim P_{data}}[\log D(y)] + E_{y \sim P_{G}}[\log (1-D(y))]
$$
